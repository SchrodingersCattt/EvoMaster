"""MatMasterAgent: finish only when task_completed=true.

System prompt is always generated by build_mat_master_system_prompt() (tool list + today's date).
"""

from __future__ import annotations

import json
import re
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from evomaster.agent.agent import Agent
from evomaster.utils.types import AssistantMessage, StepRecord, ToolMessage

# How many recent tool calls to track for loop detection
_LOOP_WINDOW = 30
# How many times the same call must appear in the window to trigger loop-break
# (1 = block on the 2nd identical call — the 1st call is the only genuine one;
#  a 2nd identical call to a deterministic tool is always wasteful.)
_LOOP_THRESHOLD = 1
# Maximum number of peek_manual search/section queries per run.
# Prevents the LLM from endlessly searching the manual with different keywords.
_PEEK_MANUAL_MAX_CALLS = 12


class MatMasterAgent(Agent):
    """Agent that only ends the run when the finish tool is called with task_completed=true.

    If the agent calls finish with task_completed=false or partial, we add the
    tool response and continue (do not set should_finish).
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Sliding window of recent tool-call fingerprints for loop detection
        self._recent_tool_fps: deque[str] = deque(maxlen=_LOOP_WINDOW)
        # Secondary window: normalised semantic fingerprints (catches near-dupes)
        self._recent_sem_fps: deque[str] = deque(maxlen=_LOOP_WINDOW)
        # Global counter for peek_manual calls (search/section/sections queries)
        self._peek_manual_call_count: int = 0

    @staticmethod
    def _tool_fingerprint(tool_call) -> str:
        """Create a hashable fingerprint for a tool call (name + canonical args).

        Uses sorted JSON keys so that identical calls with different key
        orderings (a common LLM behaviour) produce the same fingerprint.
        """
        name = tool_call.function.name
        args_str = tool_call.function.arguments or ""
        try:
            args_obj = json.loads(args_str) if args_str else {}
            canonical = json.dumps(args_obj, sort_keys=True, ensure_ascii=False)
        except (json.JSONDecodeError, TypeError):
            canonical = args_str
        return f"{name}|{canonical}"

    @staticmethod
    def _semantic_fingerprint(tool_call) -> str:
        """Create a normalised fingerprint that treats near-duplicate calls as identical.

        For use_skill calls that run peek_manual.py, we extract the key
        arguments (--software, --search, --section, --sections) and normalise
        them so that calls with different JSON key orderings or minor flag
        differences are recognised as the same query intent.
        """
        name = tool_call.function.name
        args_str = tool_call.function.arguments or ""
        try:
            args = json.loads(args_str) if args_str else {}
        except (json.JSONDecodeError, TypeError):
            args = {}

        # Normalise use_skill calls that run peek_manual.py
        script_args = args.get("script_args", "")
        script_name = args.get("script_name", "")
        if name == "use_skill" and "peek_manual" in script_name and script_args:
            # Extract key arguments to create a normalised fingerprint
            sa = script_args.upper()
            sw = ""
            sw_m = re.search(r'--SOFTWARE\s+(\S+)', sa)
            if sw_m:
                sw = sw_m.group(1)
            search_kw = ""
            m = re.search(r'--SEARCH\s+["\']?([^"\']+?)["\']?(?:\s+--|$)', sa)
            if m:
                search_kw = m.group(1).strip()
            section = ""
            m = re.search(r'--SECTION\s+["\']?([^"\']+?)["\']?(?:\s+--|$)', sa)
            if m:
                section = m.group(1).strip()
            sections = ""
            m = re.search(r'--SECTIONS\s+["\']?([^"\']+?)["\']?(?:\s+--|$)', sa)
            if m:
                sections = m.group(1).strip()
            tree = "--TREE" in sa
            return f"peek_manual|{sw}|search={search_kw}|section={section}|sections={sections}|tree={tree}"

        # Default: canonical JSON fingerprint (same as _tool_fingerprint)
        try:
            canonical = json.dumps(args, sort_keys=True, ensure_ascii=False)
        except TypeError:
            canonical = args_str
        return f"{name}|{canonical}"

    def _is_peek_manual_call(self, tool_call) -> bool:
        """Return True if this is a use_skill call that runs peek_manual.py."""
        name = tool_call.function.name
        if name != "use_skill":
            return False
        args_str = tool_call.function.arguments or ""
        try:
            args = json.loads(args_str) if args_str else {}
        except (json.JSONDecodeError, TypeError):
            args = {}
        return "peek_manual" in args.get("script_name", "")

    def _is_loop(self, tool_call) -> bool:
        """Return True if this exact or semantically-equivalent call appeared >= _LOOP_THRESHOLD times,
        or if the global peek_manual budget is exhausted."""
        # Global budget for peek_manual queries
        if self._is_peek_manual_call(tool_call) and self._peek_manual_call_count >= _PEEK_MANUAL_MAX_CALLS:
            return True
        fp = self._tool_fingerprint(tool_call)
        if self._recent_tool_fps.count(fp) >= _LOOP_THRESHOLD:
            return True
        # Also check semantic fingerprint (catches near-duplicates)
        sem_fp = self._semantic_fingerprint(tool_call)
        if self._recent_sem_fps.count(sem_fp) >= _LOOP_THRESHOLD:
            return True
        return False

    def _record_tool_call(self, tool_call) -> None:
        """Record a tool call fingerprint in the sliding window."""
        self._recent_tool_fps.append(self._tool_fingerprint(tool_call))
        self._recent_sem_fps.append(self._semantic_fingerprint(tool_call))
        if self._is_peek_manual_call(tool_call):
            self._peek_manual_call_count += 1

    def _get_system_prompt(self) -> str:
        """Use generated system prompt (tool list + date), then append working directory, tool rules, and skills.
        Date and OS/Shell are appended last so they appear at the end of the prompt (and in log tail)."""
        from ..prompts.build_prompt import build_mat_master_system_prompt

        base, current_date, os_type, shell_type = build_mat_master_system_prompt()

        working_dir = self.session.config.workspace_path
        working_dir_abs = str(Path(working_dir).absolute())
        working_dir_info = f"\n\nYou must perform all operations in this working directory; do not change directory. All file operations and commands must be run under: {working_dir_abs}"
        prompt = base + working_dir_info

        # Inject tool rules (fix once, apply every run) so repeated tool errors are avoided
        _tool_rules_path = Path(__file__).resolve().parent.parent / "prompts" / "tool_rules.txt"
        if _tool_rules_path.exists():
            prompt += "\n\n" + _tool_rules_path.read_text(encoding="utf-8").strip()

        # Mandatory citation and output format for survey/manuscript — agent MUST follow this
        _citation_format_path = Path(__file__).resolve().parent.parent / "skills" / "_common" / "reference" / "citation_and_output_format.md"
        if _citation_format_path.exists():
            prompt += "\n\n# Citation and output format (mandatory for literature surveys and manuscripts)\n\n"
            prompt += _citation_format_path.read_text(encoding="utf-8").strip()
            prompt += "\n\nYou MUST follow the above format when writing survey reports or manuscript sections: use [n](url) for every citation, include a References section with URL for each [n], and obey General / Citation / References section / Terminology rules."

        if self.skill_registry is not None:
            skills_info = self.skill_registry.get_meta_info_context()
            if skills_info:
                prompt += f"\n{skills_info}\n"
                prompt += """
You can use the 'use_skill' tool to:
1. Get detailed information about a skill: action='get_info'
2. Get reference documentation: action='get_reference'
3. Run scripts from Operator skills: action='run_script'
"""
        # Append date and OS/Shell last so they appear in the log tail (LLM logs truncate to head+tail)
        prompt += f"\nToday's date: {current_date} (OS: {os_type}, Shell: {shell_type})"
        return prompt

    def _on_assistant_message(self, msg: AssistantMessage) -> None:
        """Optional hook after assistant message is added. Override in subclasses (e.g. streaming)."""
        pass

    def _on_tool_message(self, msg: ToolMessage) -> None:
        """Optional hook after each tool message is added. Override in subclasses (e.g. streaming)."""
        pass

    def _execute_tools_parallel(
        self,
        tool_calls: list,
        *,
        max_workers: int = 4,
    ) -> list[tuple[Any, str, dict[str, Any]]]:
        """Execute multiple tool calls concurrently via ThreadPoolExecutor.

        When the LLM returns N tool calls in a single response they are
        conceptually independent, so we run them in parallel.

        Returns a list of ``(tool_call, observation, info)`` in original order.
        """
        workers = min(max_workers, len(tool_calls))
        results: dict[int, tuple[str, dict[str, Any]]] = {}

        def _run(idx: int, tc):
            obs, info = self._execute_tool(tc)
            return idx, obs, info

        if workers <= 1:
            for idx, tc in enumerate(tool_calls):
                obs, info = self._execute_tool(tc)
                results[idx] = (obs, info)
        else:
            self.logger.info(
                "Executing %d tool calls in parallel (max_workers=%d)",
                len(tool_calls), workers,
            )
            with ThreadPoolExecutor(max_workers=workers) as pool:
                futures = {
                    pool.submit(_run, idx, tc): idx
                    for idx, tc in enumerate(tool_calls)
                }
                for future in as_completed(futures):
                    idx, obs, info = future.result()
                    results[idx] = (obs, info)

        return [
            (tool_calls[i], *results[i])
            for i in range(len(tool_calls))
        ]

    def _step(self) -> bool:
        """Override: for finish tool, execute it and only set should_finish when task_completed==true."""
        self._step_count += 1

        dialog_for_query = self.context_manager.prepare_for_query(self.current_dialog)
        assistant_message = self.llm.query(dialog_for_query)
        self.current_dialog.add_message(assistant_message)
        self._on_assistant_message(assistant_message)
        step_record = StepRecord(
            step_id=self._step_count,
            assistant_message=assistant_message,
        )

        if not assistant_message.tool_calls:
            if hasattr(self, "enable_tools") and not self.enable_tools:
                self.trajectory.add_step(step_record)
                self._append_trajectory_entry(dialog_for_query, step_record)
                return True
            self._handle_no_tool_call()
            self.trajectory.add_step(step_record)
            self._append_trajectory_entry(dialog_for_query, step_record)
            return False

        should_finish = False

        # Separate finish calls from regular tool calls
        finish_call = None
        regular_calls = []
        for tool_call in assistant_message.tool_calls:
            if tool_call.function.name == "finish":
                finish_call = tool_call
            else:
                regular_calls.append(tool_call)

        # Execute regular tool calls in parallel (with loop detection)
        if regular_calls:
            # Split into executable vs loop-blocked
            exec_calls = []
            loop_blocked: list[tuple[Any, str]] = []  # (tool_call, warning_msg)
            for tc in regular_calls:
                if self._is_loop(tc):
                    # Distinguish between budget exhaustion and exact-duplicate loop
                    is_budget = (
                        self._is_peek_manual_call(tc)
                        and self._peek_manual_call_count >= _PEEK_MANUAL_MAX_CALLS
                    )
                    if is_budget:
                        self.logger.warning(
                            "BUDGET EXHAUSTED: peek_manual called %d times (max %d), skipping.",
                            self._peek_manual_call_count, _PEEK_MANUAL_MAX_CALLS,
                        )
                        msg = (
                            f"⚠️ MANUAL QUERY BUDGET EXHAUSTED: You have already called peek_manual.py "
                            f"{self._peek_manual_call_count} times (limit: {_PEEK_MANUAL_MAX_CALLS}). "
                            "ALL further manual queries are BLOCKED.\n\n"
                            "ACTION REQUIRED: STOP searching the manual. You have enough information. "
                            "Use your CP2K domain knowledge to write the input file directly NOW.\n"
                            "Many CP2K subsections (XC_FUNCTIONAL/PBE, HF, SCREENING, OT, etc.) are "
                            "valid CP2K syntax but simply not indexed in our reference JSON. "
                            "Proceed with the input file you have and submit the calculation."
                        )
                    else:
                        self.logger.warning(
                            "LOOP DETECTED: tool '%s' with same args called %d+ times, skipping.",
                            tc.function.name, _LOOP_THRESHOLD,
                        )
                        msg = (
                            f"⚠️ LOOP DETECTED: You have called '{tc.function.name}' with the exact same arguments "
                            f"{_LOOP_THRESHOLD}+ times already and received the same result each time. "
                            "This call was SKIPPED to prevent an infinite loop.\n\n"
                            "ACTION REQUIRED: Do NOT call this tool again with the same arguments. Instead:\n"
                            "1. If the parameter/section you are looking for is not in the manual, it does NOT exist. "
                            "Use your domain knowledge to write the correct syntax directly.\n"
                            "2. Try a completely DIFFERENT approach or search keyword.\n"
                            "3. If you have enough information already, proceed to write the input file NOW."
                        )
                    loop_blocked.append((tc, msg))
                else:
                    exec_calls.append(tc)
                self._record_tool_call(tc)

            # Execute non-blocked calls in parallel
            results = self._execute_tools_parallel(exec_calls) if exec_calls else []

            # Merge results: first the executed ones, then the loop-blocked ones (in original order)
            all_results: list[tuple[Any, str, dict[str, Any]]] = []
            exec_iter = iter(results)
            block_iter = iter(loop_blocked)
            for tc in regular_calls:
                if any(tc is btc for btc, _ in loop_blocked):
                    btc, warn_msg = next(block_iter)
                    all_results.append((btc, warn_msg, {"loop_blocked": True}))
                else:
                    all_results.append(next(exec_iter))

            for tool_call, observation, info in all_results:
                # Remind agent to do multiple retrievals for survey
                if tool_call.function.name == "mat_sn_search-papers-enhanced" and info.get("error") is None:
                    n_papers = ""
                    try:
                        obj = json.loads(observation)
                        if isinstance(obj, dict) and "data" in obj and isinstance(obj["data"], list):
                            n_papers = str(len(obj["data"]))
                    except (json.JSONDecodeError, TypeError):
                        pass
                    call_count = info.get("call_count", "?")
                    reminder = (
                        f"\n\n[Survey reminder: 本次返回 {n_papers or '?'} 篇（第 {call_count} 次检索）。"
                        "综述/调研需至少 6–15 次检索；若结果偏少或检索次数不足，请换 question/words 继续调用 mat_sn_search-papers-enhanced 或 mat_sn_web-search。]"
                    )
                    observation = observation + reminder

                MAX_TOOL_OUTPUT = 30000
                if len(observation) > MAX_TOOL_OUTPUT:
                    observation = (
                        observation[: MAX_TOOL_OUTPUT // 2]
                        + "\n\n... [output truncated due to length] ...\n\n"
                        + observation[-MAX_TOOL_OUTPUT // 2 :]
                    )

                tool_message = ToolMessage(
                    content=observation,
                    tool_call_id=tool_call.id,
                    name=tool_call.function.name,
                    meta={"info": info},
                )
                self.current_dialog.add_message(tool_message)
                self._on_tool_message(tool_message)
                step_record.tool_responses.append(tool_message)

        # Handle finish tool (always last, sequential)
        if finish_call:
            self.logger.debug("Processing tool call: finish")
            try:
                finish_args = json.loads(finish_call.function.arguments)
                self.logger.info("=" * 80)
                self.logger.info("Finish Tool Arguments: task_completed=%s", finish_args.get("task_completed"))
                self.logger.info("=" * 80)
            except Exception:
                pass

            observation, info = self._execute_tool(finish_call)

            MAX_TOOL_OUTPUT = 30000
            if len(observation) > MAX_TOOL_OUTPUT:
                observation = (
                    observation[: MAX_TOOL_OUTPUT // 2]
                    + "\n\n... [output truncated due to length] ...\n\n"
                    + observation[-MAX_TOOL_OUTPUT // 2 :]
                )

            task_completed = info.get("task_completed", "false")
            if task_completed == "true":
                should_finish = True

            tool_message = ToolMessage(
                content=observation,
                tool_call_id=finish_call.id,
                name=finish_call.function.name,
                meta={"info": info},
            )
            self.current_dialog.add_message(tool_message)
            self._on_tool_message(tool_message)
            step_record.tool_responses.append(tool_message)

        self.trajectory.add_step(step_record)
        self._append_trajectory_entry(dialog_for_query, step_record)
        return should_finish
