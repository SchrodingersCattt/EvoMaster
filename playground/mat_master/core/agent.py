"""MatMasterAgent: finish only when task_completed=true.

System prompt is always generated by build_mat_master_system_prompt() (tool list + today's date).
"""

from __future__ import annotations

import json
import re
from collections import deque
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from evomaster.agent.agent import Agent
from .callback import MatToolCallbacks, ToolCallbackPipeline
from .execution import BatchExecutor, ExecutionTask
from evomaster.utils.types import AssistantMessage, StepRecord, ToolMessage

# How many recent tool calls to track for loop detection
_LOOP_WINDOW = 30
# How many times the same call must appear in the window to trigger loop-break
# (1 = block on the 2nd identical call — the 1st call is the only genuine one;
#  a 2nd identical call to a deterministic tool is always wasteful.)
_LOOP_THRESHOLD = 1
# Maximum number of peek_manual search/section queries per run.
# Prevents the LLM from endlessly searching the manual with different keywords.
_PEEK_MANUAL_MAX_CALLS = 12

# Tools that are EXEMPT from loop detection and fingerprint recording.
# Job-status polling is designed to be called repeatedly with the same args
# (the caller waits for the status to change); blocking it breaks the async
# calculation workflow.
_LOOP_EXEMPT_SUFFIXES = (
    "query_job_status",
    "get_job_status",
)


class MatMasterAgent(Agent):
    """Agent that ends the run when the finish tool is called with task_completed=true or partial.

    If the agent calls finish with task_completed=false, we add the
    tool response and continue (do not set should_finish).
    """

    def __init__(self, *args, direct_max_workers: int = 4, rate_limit: float | None = None, config_dict: dict | None = None, **kwargs):
        super().__init__(*args, **kwargs)
        # Sliding window of recent tool-call fingerprints for loop detection
        self._recent_tool_fps: deque[str] = deque(maxlen=_LOOP_WINDOW)
        # Secondary window: normalised semantic fingerprints (catches near-dupes)
        self._recent_sem_fps: deque[str] = deque(maxlen=_LOOP_WINDOW)
        # Global counter for peek_manual calls (search/section/sections queries)
        self._peek_manual_call_count: int = 0
        # Concurrency config for Direct mode (BatchExecutor)
        self._direct_max_workers: int = max(1, direct_max_workers)
        self._rate_limit: float | None = rate_limit
        # Full config dict for async tool registry (prompt injection)
        self._full_config_dict: dict = config_dict or {}
        # Tool callback pipeline
        self._tool_callback_pipeline = ToolCallbackPipeline(self.logger)
        self._register_default_tool_callbacks()

    def _get_async_tool_registry(self):
        """Lazily create AsyncToolRegistry from full config dict."""
        from .async_tool_registry import AsyncToolRegistry
        return AsyncToolRegistry(self._full_config_dict)

    def _register_default_tool_callbacks(self) -> None:
        """Register default MAT callbacks in execution order."""
        MatToolCallbacks(self).register(self._tool_callback_pipeline)

    @staticmethod
    def _tool_fingerprint(tool_call) -> str:
        """Create a hashable fingerprint for a tool call (name + canonical args).

        Uses sorted JSON keys so that identical calls with different key
        orderings (a common LLM behaviour) produce the same fingerprint.
        """
        name = tool_call.function.name
        args_str = tool_call.function.arguments or ""
        try:
            args_obj = json.loads(args_str) if args_str else {}
            canonical = json.dumps(args_obj, sort_keys=True, ensure_ascii=False)
        except (json.JSONDecodeError, TypeError):
            canonical = args_str
        return f"{name}|{canonical}"

    @staticmethod
    def _semantic_fingerprint(tool_call) -> str:
        """Create a normalised fingerprint that treats near-duplicate calls as identical.

        For use_skill calls that run peek_manual.py, we extract the key
        arguments (--software, --search, --section, --sections) and normalise
        them so that calls with different JSON key orderings or minor flag
        differences are recognised as the same query intent.
        """
        name = tool_call.function.name
        args_str = tool_call.function.arguments or ""
        try:
            args = json.loads(args_str) if args_str else {}
        except (json.JSONDecodeError, TypeError):
            args = {}

        # Normalise use_skill calls that run peek_manual.py
        script_args = args.get("script_args", "")
        script_name = args.get("script_name", "")
        if name == "use_skill" and "peek_manual" in script_name and script_args:
            # Extract key arguments to create a normalised fingerprint
            sa = script_args.upper()
            sw = ""
            sw_m = re.search(r'--SOFTWARE\s+(\S+)', sa)
            if sw_m:
                sw = sw_m.group(1)
            search_kw = ""
            m = re.search(r'--SEARCH\s+["\']?([^"\']+?)["\']?(?:\s+--|$)', sa)
            if m:
                search_kw = m.group(1).strip()
            section = ""
            m = re.search(r'--SECTION\s+["\']?([^"\']+?)["\']?(?:\s+--|$)', sa)
            if m:
                section = m.group(1).strip()
            sections = ""
            m = re.search(r'--SECTIONS\s+["\']?([^"\']+?)["\']?(?:\s+--|$)', sa)
            if m:
                sections = m.group(1).strip()
            tree = "--TREE" in sa
            return f"peek_manual|{sw}|search={search_kw}|section={section}|sections={sections}|tree={tree}"

        # Default: canonical JSON fingerprint (same as _tool_fingerprint)
        try:
            canonical = json.dumps(args, sort_keys=True, ensure_ascii=False)
        except TypeError:
            canonical = args_str
        return f"{name}|{canonical}"

    def _is_peek_manual_call(self, tool_call) -> bool:
        """Return True if this is a use_skill call that runs peek_manual.py."""
        name = tool_call.function.name
        if name != "use_skill":
            return False
        args_str = tool_call.function.arguments or ""
        try:
            args = json.loads(args_str) if args_str else {}
        except (json.JSONDecodeError, TypeError):
            args = {}
        return "peek_manual" in args.get("script_name", "")

    @staticmethod
    def _is_loop_exempt(tool_call) -> bool:
        """Return True if this tool is exempt from loop detection.

        Job-status polling tools (e.g. mat_abacus_query_job_status,
        mat_binary_calc_query_job_status) are designed to be called
        repeatedly with identical arguments while waiting for a status
        change. Blocking them would break the async calculation workflow.
        """
        name = tool_call.function.name or ""
        return any(name.endswith(suffix) for suffix in _LOOP_EXEMPT_SUFFIXES)

    def _is_loop(self, tool_call) -> bool:
        """Return True if this exact or semantically-equivalent call appeared >= _LOOP_THRESHOLD times,
        or if the global peek_manual budget is exhausted.

        Loop-exempt tools (job status polling) always return False.
        """
        if self._is_loop_exempt(tool_call):
            return False
        # Global budget for peek_manual queries
        if self._is_peek_manual_call(tool_call) and self._peek_manual_call_count >= _PEEK_MANUAL_MAX_CALLS:
            return True
        fp = self._tool_fingerprint(tool_call)
        if self._recent_tool_fps.count(fp) >= _LOOP_THRESHOLD:
            return True
        # Also check semantic fingerprint (catches near-duplicates)
        sem_fp = self._semantic_fingerprint(tool_call)
        if self._recent_sem_fps.count(sem_fp) >= _LOOP_THRESHOLD:
            return True
        return False

    def _record_tool_call(self, tool_call) -> None:
        """Record a tool call fingerprint in the sliding window.

        Loop-exempt tools (job status polling) are NOT recorded so they
        don't pollute the window or count toward any thresholds.
        """
        if self._is_loop_exempt(tool_call):
            return
        self._recent_tool_fps.append(self._tool_fingerprint(tool_call))
        self._recent_sem_fps.append(self._semantic_fingerprint(tool_call))
        if self._is_peek_manual_call(tool_call):
            self._peek_manual_call_count += 1

    def _get_system_prompt(self) -> str:
        """Use generated system prompt (tool list + date), then append working directory, tool rules, and skills.
        Date and OS/Shell are appended last so they appear at the end of the prompt (and in log tail)."""
        from ..prompts.build_prompt import build_mat_master_system_prompt

        # Build registry from config for dynamic prompt injection
        registry = self._get_async_tool_registry()

        base, current_date, os_type, shell_type = build_mat_master_system_prompt(registry=registry)

        working_dir = self.session.config.workspace_path
        working_dir_abs = str(Path(working_dir).absolute())
        working_dir_info = f"\n\nYou must perform all operations in this working directory; do not change directory. All file operations and commands must be run under: {working_dir_abs}"
        prompt = base + working_dir_info

        # Inject tool rules (fix once, apply every run) so repeated tool errors are avoided.
        # Placeholders like {{ASYNC_SOFTWARE_LIST}} are replaced with registry values.
        _tool_rules_path = Path(__file__).resolve().parent.parent / "prompts" / "tool_rules.txt"
        if _tool_rules_path.exists():
            tool_rules = _tool_rules_path.read_text(encoding="utf-8").strip()
            tool_rules = registry.replace_placeholders(tool_rules)
            prompt += "\n\n" + tool_rules

        # Mandatory citation and output format for survey/manuscript — agent MUST follow this
        _citation_format_path = Path(__file__).resolve().parent.parent / "skills" / "_common" / "reference" / "citation_and_output_format.md"
        if _citation_format_path.exists():
            prompt += "\n\n# Citation and output format (mandatory for literature surveys and manuscripts)\n\n"
            prompt += _citation_format_path.read_text(encoding="utf-8").strip()
            prompt += "\n\nYou MUST follow the above format when writing survey reports or manuscript sections: use [n](url) for every citation, include a References section with URL for each [n], and obey General / Citation / References section / Terminology rules."

        if self.skill_registry is not None:
            skills_info = self.skill_registry.get_meta_info_context()
            if skills_info:
                prompt += f"\n{skills_info}\n"
                prompt += """
You can use the 'use_skill' tool to:
1. Get detailed information about a skill: action='get_info'
2. Get reference documentation: action='get_reference'
3. Run scripts from Operator skills: action='run_script'
"""
        # Append date and OS/Shell last so they appear in the log tail (LLM logs truncate to head+tail)
        prompt += f"\nToday's date: {current_date} (OS: {os_type}, Shell: {shell_type})"
        return prompt

    # ------------------------------------------------------------------
    # Observation formatting (MatMaster-only)
    # ------------------------------------------------------------------

    @staticmethod
    def _format_bash_observation(observation: str, info: dict[str, Any]) -> dict[str, Any]:
        """Build structured JSON object for ``execute_bash`` results.

        Includes a ``status`` field (``"success"`` / ``"error"``) so the LLM
        can reliably branch on command outcome.
        """
        exit_code = info.get("exit_code", -1)
        has_error = "error" in info
        if has_error:
            status = "error"
        elif exit_code != 0 and exit_code != -1:
            status = "error"
        else:
            status = "success"
        return {
            "status": status,
            "output": observation,
            "exit_code": exit_code,
            "working_dir": info.get("working_dir", ""),
        }

    @staticmethod
    def _to_json_value(value: Any) -> Any:
        """Convert observation payload to a JSON-compatible value when possible."""
        if not isinstance(value, str):
            return value
        text = value.strip()
        if not text:
            return ""
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            return value

    def _format_tool_observation(
        self, tool_name: str, observation: str, info: dict[str, Any],
    ) -> str:
        """Return JSON text for every tool observation."""
        if tool_name == "execute_bash":
            payload = self._format_bash_observation(observation, info)
        else:
            payload = {
                "status": "error" if "error" in info else "success",
                "observation": self._to_json_value(observation),
            }
            if info:
                payload["info"] = info
        return json.dumps(payload, ensure_ascii=False, indent=2, default=str)

    # ------------------------------------------------------------------
    # Tool execution
    # ------------------------------------------------------------------

    def _execute_tool(self, tool_call) -> tuple[str, dict[str, Any]]:
        """Execute tool with MAT callbacks.

        Overrides the base class so that:
        1. **All** errors include the full Python traceback.
        2. **All** observations are returned as JSON text.
        3. ``execute_bash`` results include ``status`` + command metadata.
        """
        import traceback as _tb

        try:
            self._tool_callback_pipeline.run_before(tool_call)
            # Emit tool_call event AFTER before-callbacks have patched the args,
            # so the frontend/log shows the resolved arguments (e.g. DPA model
            # alias -> OSS URL, patched bohr_job_id, etc.).
            self._on_tool_call_start(tool_call)

            tool_name = tool_call.function.name
            tool_args = tool_call.function.arguments
            self._log_tool_start(tool_name, tool_args)

            tool = self.tools.get_tool(tool_name)
            if tool is None:
                error_msg = f"Unknown tool: {tool_name}"
                self._log_tool_end(tool_name, error_msg, {"error": "tool_not_found"})
                obs, inf = self._tool_callback_pipeline.run_after(
                    tool_call, error_msg, {"error": "tool_not_found"}
                )
                return self._format_tool_observation(tool_name, obs, inf), inf

            try:
                observation, info = tool.execute(self.session, tool_args)
                self._log_tool_end(tool_name, observation, info)
            except Exception as e:
                tb_str = _tb.format_exc()
                error_msg = f"Tool execution error: {e}\n\nTraceback:\n{tb_str}"
                self.logger.error("Tool execution failed:\n%s", tb_str)
                self._log_tool_end(tool_name, error_msg, {"error": str(e)})
                observation, info = error_msg, {"error": str(e)}

            observation, info = self._tool_callback_pipeline.run_after(
                tool_call, observation, info,
            )
            return self._format_tool_observation(tool_name, observation, info), info

        except Exception as exc:
            # Catch-all: callback pipeline or any other unexpected error
            tb_str = _tb.format_exc()
            error_msg = f"Tool execution error: {exc}\n\nTraceback:\n{tb_str}"
            self.logger.error("_execute_tool failed:\n%s", tb_str)
            return self._format_tool_observation(
                "internal_error", error_msg, {"error": str(exc)},
            ), {"error": str(exc)}

    def _on_assistant_message(self, msg: AssistantMessage) -> None:
        """Optional hook after assistant message is added. Override in subclasses (e.g. streaming)."""
        pass

    def _on_tool_call_start(self, tool_call) -> None:
        """Optional hook called after before-callbacks have patched tool_call
        args but before the tool is actually executed.

        Override in subclasses (e.g. StreamingMatMasterAgent) to emit
        ``tool_call`` events with the callback-resolved arguments."""
        pass

    def _on_tool_message(self, msg: ToolMessage) -> None:
        """Optional hook after each tool message is added. Override in subclasses (e.g. streaming)."""
        pass

    def _execute_tools_parallel(
        self,
        tool_calls: list,
        *,
        max_workers: int = 4,
    ) -> list[tuple[Any, str, dict[str, Any]]]:
        """Execute multiple tool calls concurrently via the unified BatchExecutor.

        When the LLM returns N tool calls in a single response they are
        conceptually independent, so we run them in parallel.

        Returns a list of ``(tool_call, observation, info)`` in original order.
        """
        if not tool_calls:
            return []

        # Build ExecutionTask list — each task wraps _execute_tool for one tool_call
        batch_tasks: list[ExecutionTask] = []
        for idx, tc in enumerate(tool_calls):
            batch_tasks.append(
                ExecutionTask(
                    task_id=str(idx),
                    func=self._execute_tool,
                    kwargs={"tool_call": tc},
                    meta={"tool_call_index": idx},
                )
            )

        # Use shared BatchExecutor (true concurrency for I/O-bound tool calls)
        executor = BatchExecutor(max_workers=max_workers, rate_limit=self._rate_limit)
        results = executor.execute_batch(batch_tasks)

        # Map results back to (tool_call, observation, info) triples
        ordered: list[tuple[Any, str, dict[str, Any]]] = []
        for idx, res in enumerate(results):
            tc = tool_calls[idx]
            if res.status == "success":
                ordered.append((tc, res.output, res.info))
            else:
                # On failure, surface the error message as the observation
                ordered.append((tc, res.output or res.error or "Unknown error", res.info))
        return ordered

    def _step(self) -> bool:
        """Override: for finish tool, execute it and set should_finish when task_completed is true or partial."""
        self._step_count += 1

        dialog_for_query = self.context_manager.prepare_for_query(self.current_dialog)
        assistant_message = self.llm.query(dialog_for_query)
        self.current_dialog.add_message(assistant_message)
        self._on_assistant_message(assistant_message)
        step_record = StepRecord(
            step_id=self._step_count,
            assistant_message=assistant_message,
        )

        if not assistant_message.tool_calls:
            if hasattr(self, "enable_tools") and not self.enable_tools:
                self.trajectory.add_step(step_record)
                self._append_trajectory_entry(dialog_for_query, step_record)
                return True
            self._handle_no_tool_call()
            self.trajectory.add_step(step_record)
            self._append_trajectory_entry(dialog_for_query, step_record)
            return False

        should_finish = False

        # Separate finish calls from regular tool calls
        finish_call = None
        regular_calls = []
        for tool_call in assistant_message.tool_calls:
            if tool_call.function.name == "finish":
                finish_call = tool_call
            else:
                regular_calls.append(tool_call)

        # Execute regular tool calls in parallel (with loop detection)
        if regular_calls:
            # Split into executable vs loop-blocked
            exec_calls = []
            loop_blocked: list[tuple[Any, str]] = []  # (tool_call, warning_msg)
            for tc in regular_calls:
                if self._is_loop(tc):
                    # Distinguish between budget exhaustion and exact-duplicate loop
                    is_budget = (
                        self._is_peek_manual_call(tc)
                        and self._peek_manual_call_count >= _PEEK_MANUAL_MAX_CALLS
                    )
                    if is_budget:
                        self.logger.warning(
                            "BUDGET EXHAUSTED: peek_manual called %d times (max %d), skipping.",
                            self._peek_manual_call_count, _PEEK_MANUAL_MAX_CALLS,
                        )
                        msg = (
                            f"⚠️ MANUAL QUERY BUDGET EXHAUSTED: You have already called peek_manual.py "
                            f"{self._peek_manual_call_count} times (limit: {_PEEK_MANUAL_MAX_CALLS}). "
                            "ALL further manual queries are BLOCKED.\n\n"
                            "ACTION REQUIRED: STOP searching the manual. You have enough information. "
                            "Use your CP2K domain knowledge to write the input file directly NOW.\n"
                            "Many CP2K subsections (XC_FUNCTIONAL/PBE, HF, SCREENING, OT, etc.) are "
                            "valid CP2K syntax but simply not indexed in our reference JSON. "
                            "Proceed with the input file you have and submit the calculation."
                        )
                    else:
                        self.logger.warning(
                            "LOOP DETECTED: tool '%s' with same args called %d+ times, skipping.",
                            tc.function.name, _LOOP_THRESHOLD,
                        )
                        msg = (
                            f"⚠️ LOOP DETECTED: You have called '{tc.function.name}' with the exact same arguments "
                            f"{_LOOP_THRESHOLD}+ times already and received the same result each time. "
                            "This call was SKIPPED to prevent an infinite loop.\n\n"
                            "ACTION REQUIRED: Do NOT call this tool again with the same arguments. Instead:\n"
                            "1. If the parameter/section you are looking for is not in the manual, it does NOT exist. "
                            "Use your domain knowledge to write the correct syntax directly.\n"
                            "2. Try a completely DIFFERENT approach or search keyword.\n"
                            "3. If you have enough information already, proceed to write the input file NOW."
                        )
                    loop_blocked.append((tc, msg))
                    # Emit tool_call event for blocked calls too (with original
                    # args — no before-callback ran since the tool is skipped).
                    self._on_tool_call_start(tc)
                else:
                    exec_calls.append(tc)
                self._record_tool_call(tc)

            # Execute non-blocked calls in parallel
            results = self._execute_tools_parallel(exec_calls, max_workers=self._direct_max_workers) if exec_calls else []

            # Merge results: first the executed ones, then the loop-blocked ones (in original order)
            all_results: list[tuple[Any, str, dict[str, Any]]] = []
            exec_iter = iter(results)
            block_iter = iter(loop_blocked)
            for tc in regular_calls:
                if any(tc is btc for btc, _ in loop_blocked):
                    btc, warn_msg = next(block_iter)
                    all_results.append((btc, warn_msg, {"loop_blocked": True}))
                else:
                    all_results.append(next(exec_iter))

            for tool_call, observation, info in all_results:
                # Full content for streaming (yield) and trajectory recording
                tool_message = ToolMessage(
                    content=observation,
                    tool_call_id=tool_call.id,
                    name=tool_call.function.name,
                    meta={"info": info},
                )
                self._on_tool_message(tool_message)
                step_record.tool_responses.append(tool_message)

                # For LLM dialog: truncate if too long to prevent context overflow
                # (naive mid-string split may break JSON, but LLM only needs gist)
                MAX_TOOL_OUTPUT = 30000
                if len(observation) > MAX_TOOL_OUTPUT:
                    observation_for_llm = (
                        observation[: MAX_TOOL_OUTPUT // 2]
                        + "\n\n... [output truncated due to length] ...\n\n"
                        + observation[-MAX_TOOL_OUTPUT // 2 :]
                    )
                    dialog_message = ToolMessage(
                        content=observation_for_llm,
                        tool_call_id=tool_call.id,
                        name=tool_call.function.name,
                        meta={"info": info},
                    )
                    self.current_dialog.add_message(dialog_message)
                else:
                    self.current_dialog.add_message(tool_message)

        # Handle finish tool (always last, sequential)
        if finish_call:
            self.logger.debug("Processing tool call: finish")
            try:
                finish_args = json.loads(finish_call.function.arguments)
                self.logger.info("=" * 80)
                self.logger.info("Finish Tool Arguments: task_completed=%s", finish_args.get("task_completed"))
                self.logger.info("=" * 80)
            except Exception:
                pass

            observation, info = self._execute_tool(finish_call)

            task_completed = info.get("task_completed", "false")
            if task_completed in ("true", "partial"):
                should_finish = True

            # Full content for streaming (yield) and trajectory recording
            tool_message = ToolMessage(
                content=observation,
                tool_call_id=finish_call.id,
                name=finish_call.function.name,
                meta={"info": info},
            )
            self._on_tool_message(tool_message)
            step_record.tool_responses.append(tool_message)

            # For LLM dialog: truncate if too long to prevent context overflow
            MAX_TOOL_OUTPUT = 30000
            if len(observation) > MAX_TOOL_OUTPUT:
                observation_for_llm = (
                    observation[: MAX_TOOL_OUTPUT // 2]
                    + "\n\n... [output truncated due to length] ...\n\n"
                    + observation[-MAX_TOOL_OUTPUT // 2 :]
                )
                dialog_message = ToolMessage(
                    content=observation_for_llm,
                    tool_call_id=finish_call.id,
                    name=finish_call.function.name,
                    meta={"info": info},
                )
                self.current_dialog.add_message(dialog_message)
            else:
                self.current_dialog.add_message(tool_message)

        self.trajectory.add_step(step_record)
        self._append_trajectory_entry(dialog_for_query, step_record)
        return should_finish
