"""MatMasterAgent: finish only when task_completed=true.

System prompt is always generated by build_mat_master_system_prompt() (tool list + today's date).
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any

from evomaster.agent.agent import Agent
from .async_execution_policy import AsyncExecutionPolicy
from .callback import MatToolCallbacks, ToolCallbackPipeline
from .job_registry import JobRegistry
from .execution import BatchExecutor, ExecutionTask
from .tool_guard import ToolGuard
from evomaster.utils.types import AssistantMessage, StepRecord, ToolMessage

class MatMasterAgent(Agent):
    """Agent that ends the run when the finish tool is called with task_completed=true or partial.

    If the agent calls finish with task_completed=false, we add the
    tool response and continue (do not set should_finish).
    """

    def __init__(self, *args, direct_max_workers: int = 4, rate_limit: float | None = None, config_dict: dict | None = None, **kwargs):
        super().__init__(*args, **kwargs)
        # Stateful guard for loop prevention + validation gate.
        self._tool_guard = ToolGuard(self.logger)
        # Concurrency config for Direct mode (BatchExecutor)
        self._direct_max_workers: int = max(1, direct_max_workers)
        self._rate_limit: float | None = rate_limit
        # Full config dict for async tool registry (prompt injection)
        self._full_config_dict: dict = config_dict or {}
        from .async_tool_registry import AsyncToolRegistry
        self._async_tool_registry = AsyncToolRegistry(self._full_config_dict)
        self._async_execution_policy = AsyncExecutionPolicy(self._async_tool_registry)
        # Runtime-tracked async jobs: source of truth for finish-attempt gate.
        self._job_registry = JobRegistry(self.logger)
        # Tool callback pipeline
        self._tool_callback_pipeline = ToolCallbackPipeline(self.logger)
        self._register_default_tool_callbacks()

    def _get_async_tool_registry(self):
        """Get async registry derived from full config dict."""
        return self._async_tool_registry

    def _register_default_tool_callbacks(self) -> None:
        """Register default MAT callbacks in execution order."""
        MatToolCallbacks(self).register(self._tool_callback_pipeline)

    def _get_system_prompt(self) -> str:
        """Use generated system prompt (tool list + date), then append working directory, tool rules, and skills.
        Date and OS/Shell are appended last so they appear at the end of the prompt (and in log tail)."""
        from ..prompts.build_prompt import build_mat_master_system_prompt

        # Build registry from config for dynamic prompt injection
        registry = self._get_async_tool_registry()

        base, current_date, os_type, shell_type = build_mat_master_system_prompt(registry=registry)

        working_dir = self.session.config.workspace_path
        working_dir_abs = str(Path(working_dir).absolute())
        working_dir_info = f"\n\nYou must perform all operations in this working directory; do not change directory. All file operations and commands must be run under: {working_dir_abs}"
        prompt = base + working_dir_info

        # Inject tool rules (fix once, apply every run) so repeated tool errors are avoided.
        # Placeholders like {{ASYNC_SOFTWARE_LIST}} are replaced with registry values.
        _tool_rules_path = Path(__file__).resolve().parent.parent / "prompts" / "tool_rules.txt"
        if _tool_rules_path.exists():
            tool_rules = _tool_rules_path.read_text(encoding="utf-8").strip()
            tool_rules = registry.replace_placeholders(tool_rules)
            prompt += "\n\n" + tool_rules

        # Mandatory citation and output format for survey/manuscript — agent MUST follow this
        _citation_format_path = Path(__file__).resolve().parent.parent / "skills" / "_common" / "reference" / "citation_and_output_format.md"
        if _citation_format_path.exists():
            prompt += "\n\n# Citation and output format (mandatory for literature surveys and manuscripts)\n\n"
            prompt += _citation_format_path.read_text(encoding="utf-8").strip()
            prompt += "\n\nYou MUST follow the above format when writing survey reports or manuscript sections: use [n](url) for every citation, include a References section with URL for each [n], and obey General / Citation / References section / Terminology rules."

        if self.skill_registry is not None:
            skills_info = self.skill_registry.get_meta_info_context()
            if skills_info:
                prompt += f"\n{skills_info}\n"
                prompt += """
You can use the 'use_skill' tool to:
1. Get detailed information about a skill: action='get_info'
2. Get reference documentation: action='get_reference'
3. Run scripts from Operator skills: action='run_script'
"""
        # Append date and OS/Shell last so they appear in the log tail (LLM logs truncate to head+tail)
        prompt += f"\nToday's date: {current_date} (OS: {os_type}, Shell: {shell_type})"
        return prompt

    def _get_tool_specs(self) -> list:
        """Expose MCP tools using unified async execution policy."""
        specs = super()._get_tool_specs()
        return self._async_execution_policy.filter_tool_specs_for_llm(specs)

    # ------------------------------------------------------------------
    # Observation formatting (MatMaster-only)
    # ------------------------------------------------------------------

    @staticmethod
    def _format_bash_observation(observation: str, info: dict[str, Any]) -> dict[str, Any]:
        """Build structured JSON object for ``execute_bash`` results.

        Includes a ``status`` field (``"success"`` / ``"error"``) so the LLM
        can reliably branch on command outcome.
        """
        exit_code = info.get("exit_code", -1)
        has_error = "error" in info
        if has_error:
            status = "error"
        elif exit_code != 0 and exit_code != -1:
            status = "error"
        else:
            status = "success"
        return {
            "status": status,
            "output": observation,
            "exit_code": exit_code,
            "working_dir": info.get("working_dir", ""),
        }

    @staticmethod
    def _to_json_value(value: Any) -> Any:
        """Convert observation payload to a JSON-compatible value when possible."""
        if not isinstance(value, str):
            return value
        text = value.strip()
        if not text:
            return ""
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            return value

    def _format_tool_observation(
        self, tool_name: str, observation: str, info: dict[str, Any],
    ) -> str:
        """Return JSON text for every tool observation."""
        if tool_name == "execute_bash":
            payload = self._format_bash_observation(observation, info)
        else:
            status = "error" if "error" in info else "success"
            # For use_skill(action=run_script), propagate non-zero script exit code
            # to outer status so tool-level status matches business outcome.
            if (
                tool_name == "use_skill"
                and info.get("action") == "run_script"
                and isinstance(info.get("exit_code"), int)
                and info["exit_code"] != 0
            ):
                status = "error"
            payload = {
                "status": status,
                "observation": self._to_json_value(observation),
            }
            if info:
                payload["info"] = info
        return json.dumps(payload, ensure_ascii=False, indent=2, default=str)

    # ------------------------------------------------------------------
    # Tool execution
    # ------------------------------------------------------------------

    def _execute_tool(self, tool_call) -> tuple[str, dict[str, Any]]:
        """Execute tool with MAT callbacks.

        Overrides the base class so that:
        1. **All** errors include the full Python traceback.
        2. **All** observations are returned as JSON text.
        3. ``execute_bash`` results include ``status`` + command metadata.
        """
        import traceback as _tb

        try:
            self._tool_callback_pipeline.run_before(tool_call)
            # Emit tool_call event AFTER before-callbacks have patched the args,
            # so the frontend/log shows the resolved arguments (e.g. DPA model
            # alias -> OSS URL, patched bohr_job_id, etc.).
            self._on_tool_call_start(tool_call)

            tool_name = tool_call.function.name
            tool_args = tool_call.function.arguments
            self._log_tool_start(tool_name, tool_args)

            tool = self.tools.get_tool(tool_name)
            if tool is None:
                error_msg = f"Unknown tool: {tool_name}"
                self._log_tool_end(tool_name, error_msg, {"error": "tool_not_found"})
                obs, inf = self._tool_callback_pipeline.run_after(
                    tool_call, error_msg, {"error": "tool_not_found"}
                )
                return self._format_tool_observation(tool_name, obs, inf), inf

            try:
                observation, info = tool.execute(self.session, tool_args)
                self._log_tool_end(tool_name, observation, info)
            except Exception as e:
                tb_str = _tb.format_exc()
                error_msg = f"Tool execution error: {e}\n\nTraceback:\n{tb_str}"
                self.logger.error("Tool execution failed:\n%s", tb_str)
                self._log_tool_end(tool_name, error_msg, {"error": str(e)})
                observation, info = error_msg, {"error": str(e)}

            observation, info = self._tool_callback_pipeline.run_after(
                tool_call, observation, info,
            )
            return self._format_tool_observation(tool_name, observation, info), info

        except Exception as exc:
            # Catch-all: callback pipeline or any other unexpected error
            tb_str = _tb.format_exc()
            error_msg = f"Tool execution error: {exc}\n\nTraceback:\n{tb_str}"
            self.logger.error("_execute_tool failed:\n%s", tb_str)
            return self._format_tool_observation(
                "internal_error", error_msg, {"error": str(exc)},
            ), {"error": str(exc)}

    def _on_assistant_message(self, msg: AssistantMessage) -> None:
        """Optional hook after assistant message is added. Override in subclasses (e.g. streaming)."""
        pass

    def _on_tool_call_start(self, tool_call) -> None:
        """Optional hook called after before-callbacks have patched tool_call
        args but before the tool is actually executed.

        Override in subclasses (e.g. StreamingMatMasterAgent) to emit
        ``tool_call`` events with the callback-resolved arguments."""
        pass

    def _on_tool_message(self, msg: ToolMessage) -> None:
        """Optional hook after each tool message is added. Override in subclasses (e.g. streaming)."""
        pass

    def _execute_tools_parallel(
        self,
        tool_calls: list,
        *,
        max_workers: int = 4,
    ) -> list[tuple[Any, str, dict[str, Any]]]:
        """Execute multiple tool calls concurrently via the unified BatchExecutor.

        When the LLM returns N tool calls in a single response they are
        conceptually independent, so we run them in parallel.

        Returns a list of ``(tool_call, observation, info)`` in original order.
        """
        if not tool_calls:
            return []

        # Build ExecutionTask list — each task wraps _execute_tool for one tool_call
        batch_tasks: list[ExecutionTask] = []
        for idx, tc in enumerate(tool_calls):
            batch_tasks.append(
                ExecutionTask(
                    task_id=str(idx),
                    func=self._execute_tool,
                    kwargs={"tool_call": tc},
                    meta={"tool_call_index": idx},
                )
            )

        # Use shared BatchExecutor (true concurrency for I/O-bound tool calls)
        executor = BatchExecutor(max_workers=max_workers, rate_limit=self._rate_limit)
        results = executor.execute_batch(batch_tasks)

        # Map results back to (tool_call, observation, info) triples
        ordered: list[tuple[Any, str, dict[str, Any]]] = []
        for idx, res in enumerate(results):
            tc = tool_calls[idx]
            if res.status == "success":
                ordered.append((tc, res.output, res.info))
            else:
                # On failure, surface the error message as the observation
                ordered.append((tc, res.output or res.error or "Unknown error", res.info))
        return ordered

    def _step(self) -> bool:
        """Override: for finish tool, execute it and set should_finish when task_completed is true or partial."""
        self._step_count += 1
        # Keep async monitor state fresh across turns.
        self._job_registry.refresh_pending()

        dialog_for_query = self.context_manager.prepare_for_query(self.current_dialog)
        assistant_message = self.llm.query(dialog_for_query)
        self.current_dialog.add_message(assistant_message)
        self._on_assistant_message(assistant_message)
        step_record = StepRecord(
            step_id=self._step_count,
            assistant_message=assistant_message,
        )

        if not assistant_message.tool_calls:
            if hasattr(self, "enable_tools") and not self.enable_tools:
                self.trajectory.add_step(step_record)
                self._append_trajectory_entry(dialog_for_query, step_record)
                return True
            self._handle_no_tool_call()
            self.trajectory.add_step(step_record)
            self._append_trajectory_entry(dialog_for_query, step_record)
            return False

        should_finish = False

        # Separate finish calls from regular tool calls
        finish_call = None
        regular_calls = []
        for tool_call in assistant_message.tool_calls:
            if tool_call.function.name == "finish":
                finish_call = tool_call
            else:
                regular_calls.append(tool_call)

        # Execute regular tool calls in parallel (with loop detection)
        if regular_calls:
            # Split into executable vs loop-blocked
            exec_calls = []
            loop_blocked: list[tuple[Any, str]] = []  # (tool_call, warning_msg)
            pending_jobs_exist = bool(self._job_registry.pending_jobs())
            for tc in regular_calls:
                if pending_jobs_exist and not self._async_execution_policy.is_call_allowed_while_pending(tc):
                    loop_blocked.append((
                        tc,
                        self._async_execution_policy.pending_gate_message(),
                    ))
                    self._on_tool_call_start(tc)
                    self._tool_guard.record_tool_call(tc)
                    continue
                decision = self._tool_guard.evaluate(tc)
                if decision.blocked:
                    loop_blocked.append((tc, decision.message))
                    # Emit tool_call event for blocked calls too (with original args).
                    self._on_tool_call_start(tc)
                else:
                    exec_calls.append(tc)
                self._tool_guard.record_tool_call(tc)

            # Execute non-blocked calls in parallel
            results = self._execute_tools_parallel(exec_calls, max_workers=self._direct_max_workers) if exec_calls else []

            # Merge results: first the executed ones, then the loop-blocked ones (in original order)
            all_results: list[tuple[Any, str, dict[str, Any]]] = []
            exec_iter = iter(results)
            block_iter = iter(loop_blocked)
            for tc in regular_calls:
                if any(tc is btc for btc, _ in loop_blocked):
                    btc, warn_msg = next(block_iter)
                    all_results.append((btc, warn_msg, {"loop_blocked": True}))
                else:
                    all_results.append(next(exec_iter))

            for tool_call, observation, info in all_results:
                self._tool_guard.update_after_tool(tool_call, observation, info)
                # Full content for streaming (yield) and trajectory recording
                tool_message = ToolMessage(
                    content=observation,
                    tool_call_id=tool_call.id,
                    name=tool_call.function.name,
                    meta={"info": info},
                )
                self._on_tool_message(tool_message)
                step_record.tool_responses.append(tool_message)

                # For LLM dialog: truncate if too long to prevent context overflow
                # (naive mid-string split may break JSON, but LLM only needs gist)
                MAX_TOOL_OUTPUT = 30000
                if len(observation) > MAX_TOOL_OUTPUT:
                    observation_for_llm = (
                        observation[: MAX_TOOL_OUTPUT // 2]
                        + "\n\n... [output truncated due to length] ...\n\n"
                        + observation[-MAX_TOOL_OUTPUT // 2 :]
                    )
                    dialog_message = ToolMessage(
                        content=observation_for_llm,
                        tool_call_id=tool_call.id,
                        name=tool_call.function.name,
                        meta={"info": info},
                    )
                    self.current_dialog.add_message(dialog_message)
                else:
                    self.current_dialog.add_message(tool_message)

        # Handle finish tool (always last, sequential)
        if finish_call:
            self.logger.debug("Processing tool call: finish")
            try:
                finish_args = json.loads(finish_call.function.arguments)
                self.logger.info("=" * 80)
                self.logger.info("Finish Tool Arguments: task_completed=%s", finish_args.get("task_completed"))
                self.logger.info("=" * 80)
            except Exception:
                pass

            observation, info = self._execute_tool(finish_call)

            task_completed = info.get("task_completed", "false")
            if task_completed in ("true", "partial"):
                self._job_registry.refresh_pending()
                can_finish, gate_info = self._job_registry.can_finish()
                info = dict(info or {})
                info.update(gate_info)
                if can_finish:
                    should_finish = True
                else:
                    should_finish = False
                    observation = (
                        f"{observation}\n\n"
                        "[finish_attempt_gate] Blocked: pending async jobs still running. "
                        "Continue monitoring until pending_jobs_check passes."
                    )

            # Full content for streaming (yield) and trajectory recording
            tool_message = ToolMessage(
                content=observation,
                tool_call_id=finish_call.id,
                name=finish_call.function.name,
                meta={"info": info},
            )
            self._on_tool_message(tool_message)
            step_record.tool_responses.append(tool_message)

            # For LLM dialog: truncate if too long to prevent context overflow
            MAX_TOOL_OUTPUT = 30000
            if len(observation) > MAX_TOOL_OUTPUT:
                observation_for_llm = (
                    observation[: MAX_TOOL_OUTPUT // 2]
                    + "\n\n... [output truncated due to length] ...\n\n"
                    + observation[-MAX_TOOL_OUTPUT // 2 :]
                )
                dialog_message = ToolMessage(
                    content=observation_for_llm,
                    tool_call_id=finish_call.id,
                    name=finish_call.function.name,
                    meta={"info": info},
                )
                self.current_dialog.add_message(dialog_message)
            else:
                self.current_dialog.add_message(tool_message)

        self.trajectory.add_step(step_record)
        self._append_trajectory_entry(dialog_for_query, step_record)
        return should_finish
