# LLM 配置示例
llm:
  # OpenAI 兼容的 LLM 配置
  openai:
    provider: "openai"
    model: "gpt-5"
    api_key: "sk-5LbA2T3ixhf09JpXzzvpQchzCDK6Di9Eh5txc6sGOfas9Spj"
    base_url: "http://123.129.219.111:3000/v1"
    temperature: 0.7
    max_tokens: 16384
    timeout: 60
    max_retries: 3
    retry_delay: 1.0

  # 自定义本地 vLLM（使用 Qwen 模型）
  local_vllm:
    provider: "openai"
    model: "Qwen/Qwen2.5-7B-Instruct"
    api_key: "sk-"
    base_url: "http://123.129.219.111:3000/v1"
    temperature: 0.8
    max_tokens: 16384
    timeout: 30
    max_retries: 2
    retry_delay: 0.5

  # Anthropic Claude 配置
  anthropic:
    provider: "anthropic"
    model: "claude-haiku-4-5-20251001"
    api_key: "sk-5LbA2T3ixhf09JpXzzvpQchzCDK6Di9Eh5txc6sGOfas9Spj"
    base_url: "http://123.129.219.111:3000"
    temperature: 0.7
    max_tokens: 16384
    timeout: 60
    max_retries: 3
    retry_delay: 1.0

  # 默认使用的 LLM 配置名称
  default: "openai"
